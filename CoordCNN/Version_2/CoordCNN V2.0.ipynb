{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Cl3-HUn9UH2y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scaled_trainer(arr, lb, lf, choice):\n",
    "    sc = MinMaxScaler(feature_range=(-1,1))\n",
    "    tx = np.array([])\n",
    "    ty = np.array([])\n",
    "    t = np.array([])\n",
    "    tx_new = np.array([])\n",
    "\n",
    "    for i in arr:\n",
    "        data = pd.read_csv(i).filter([choice]).values\n",
    "        data = sc.fit_transform(data)\n",
    "        t = np.append(t, sliding_window_view(data, window_shape = (lb+lf,1)))\n",
    "\n",
    "    t = t.reshape(int(t.shape[0]/(lb+lf)), lb+lf)\n",
    "    for j in t:\n",
    "        tx = np.append(tx, j[:lb])\n",
    "        ty = np.append(ty, j[lb:])\n",
    "    tx = tx.reshape((int(tx.shape[0]/lb), lb))\n",
    "       \n",
    "    tx_new = tx_new.reshape((int(tx_new.shape[0]/(lb-1)), (lb-1)))\n",
    "    ty = ty.reshape((int(ty.shape[0]/lf), lf))\n",
    "\n",
    "    return tx, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "myJ3Ztk1zHMp",
    "outputId": "a16661e4-f7c7-4371-fd2b-d19f3dc7636b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc13162d4d09>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrade_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No. of Trades\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mshl_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Spread High-Low\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mwap_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwap_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WAP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c0a7473e92f2>\u001b[0m in \u001b[0;36mscaled_trainer\u001b[0;34m(arr, lb, lf, choice)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msliding_window_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'asian paints.csv'"
     ]
    }
   ],
   "source": [
    "lb=200\n",
    "lf=5\n",
    "#List of companies to train on:\n",
    "companies = [\"asian paints.csv\", \"edelweiss.csv\", \"infosys.csv\", \"reliance industries.csv\", \n",
    "             \"reliance power.csv\", \"tata chemicals.csv\", \"tata coffee.csv\", \n",
    "             \"tata motors.csv\", \"tata steel.csv\"]\n",
    "\n",
    "for com in companies:\n",
    "    trade_x, _ = scaled_trainer([com], lb, lf, \"No. of Trades\")\n",
    "    shl_x, _ = scaled_trainer([com], lb, lf, \"Spread High-Low\")\n",
    "    wap_x, wap_y = scaled_trainer([com], lb, lf, \"WAP\")\n",
    "    open_x, _y = scaled_trainer([com], lb, lf, \"Open Price\")\n",
    "    sco_x, _ = scaled_trainer([com], lb, lf, \"Spread Close-Open\")\n",
    "    \n",
    "    wap_img = np.array([])\n",
    "    shl_img = np.array([])\n",
    "    trade_img = np.array([])\n",
    "    sco_img = np.array([])\n",
    "    open_img = np.array([])\n",
    "    for i in range(len(wap_x)):\n",
    "        eps = 0.1\n",
    "        d1 = np.abs(np.subtract.outer(wap_x[i],wap_x[i]))\n",
    "        R1 = np.exp(-d1**2/eps)\n",
    "        wap_img = np.append(wap_img, [R1])\n",
    "        d2 = np.abs(np.subtract.outer(shl_x[i],shl_x[i]))\n",
    "        R2 = np.exp(-d2**2/eps)\n",
    "        shl_img = np.append(shl_img, [R2])\n",
    "        d3 = np.abs(np.subtract.outer(trade_x[i],trade_x[i]))\n",
    "        R3 = np.exp(-d3**2/eps)\n",
    "        trade_img = np.append(trade_img, [R3]) \n",
    "        d4 = np.abs(np.subtract.outer(sco_x[i],sco_x[i]))\n",
    "        R4 = np.exp(-d3**2/eps)\n",
    "        sco_img = np.append(sco_img, [R4]) \n",
    "        d5 = np.abs(np.subtract.outer(open_x[i],open_x[i]))\n",
    "        R5 = np.exp(-d3**2/eps)\n",
    "        open_img = np.append(open_img, [R5]) \n",
    "        \n",
    "    wap_img = wap_img.reshape((int(wap_img.shape[0]/(lb*lb)), lb, lb, 1))\n",
    "    shl_img = shl_img.reshape((int(shl_img.shape[0]/(lb*lb)), lb, lb, 1))\n",
    "    trade_img = trade_img.reshape((int(trade_img.shape[0]/(lb*lb)), lb, lb, 1))\n",
    "    sco_img = sco_img.reshape((int(sco_img.shape[0]/(lb*lb)), lb, lb, 1))\n",
    "    open_img = open_img.reshape((int(open_img.shape[0]/(lb*lb)), lb, lb, 1))\n",
    "\n",
    "    x = [1,2,3,4,5]\n",
    "    slopes = np.array([])\n",
    "    for i in wap_y:\n",
    "        slope, _ = np.polyfit(x, i, 1)\n",
    "        slopes = np.append(slopes, slope)\n",
    "\n",
    "    model_out = np.array([])\n",
    "    for i in slopes:\n",
    "        if i<(-0.01):\n",
    "            model_out = np.append(model_out, 0)\n",
    "        elif i<=(0.01):\n",
    "            model_out = np.append(model_out, 1)\n",
    "        else:\n",
    "            model_out = np.append(model_out, 2)\n",
    "    \n",
    "    bins = np.arange(0, 4)\n",
    "    hist, _ = np.histogram(slopes, bins=3)\n",
    "    print(hist)\n",
    "    plt.bar(bins[:-1], hist, align='center')\n",
    "    plt.xticks(bins[:-1])\n",
    "    plt.show()\n",
    "\n",
    "    model_in = wap_img\n",
    "    model_in = np.append(model_in, open_img, axis=3)\n",
    "    model_in = np.append(model_in, shl_img, axis=3)\n",
    "    model_in = np.append(model_in, sco_img, axis=3)\n",
    "    model_in = np.append(model_in, trade_img, axis=3)\n",
    "\n",
    "    np.save(f\"input {com[:-4]}\", model_in)\n",
    "    np.save(f\"output {com[:-4]}\", model_out)\n",
    "    print(f\"Saved data on {com[:-4]}: Input/Output\", model_in.shape, model_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0eKPlGCI3ri"
   },
   "outputs": [],
   "source": [
    "lb=200\n",
    "lf=5\n",
    "#List of companies to train on:\n",
    "companies = [\"asian paints.csv\", \"edelweiss.csv\", \"infosys.csv\", \"reliance industries.csv\", \n",
    "             \"reliance power.csv\", \"tata chemicals.csv\", \"tata coffee.csv\", \n",
    "             \"tata motors.csv\", \"tata steel.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEA72sIwgpd9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import base\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class AddCoords(base.Layer):\n",
    "    \"\"\"Add coords to a tensor\"\"\"\n",
    "    def __init__(self, x_dim=lb, y_dim=lb, with_r=False):\n",
    "        super(AddCoords, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.with_r = with_r\n",
    "    def call(self, input_tensor):\n",
    "        \"\"\"\n",
    "        input_tensor: (batch, x_dim, y_dim, c)\n",
    "        \"\"\"\n",
    "        batch_size_tensor = tf.shape(input_tensor)[0]\n",
    "        xx_ones = tf.ones([batch_size_tensor, self.x_dim], dtype=tf.int32)\n",
    "        xx_ones = tf.expand_dims(xx_ones, -1)\n",
    "        xx_range = tf.tile(tf.expand_dims(tf.range(self.y_dim), 0), [batch_size_tensor, 1])\n",
    "        xx_range = tf.expand_dims(xx_range, 1)\n",
    "        xx_channel = tf.matmul(xx_ones, xx_range)\n",
    "        xx_channel = tf.expand_dims(xx_channel, -1)\n",
    "        yy_ones = tf.ones([batch_size_tensor, self.y_dim], dtype=tf.int32)\n",
    "        yy_ones = tf.expand_dims(yy_ones, 1)\n",
    "        yy_range = tf.tile(tf.expand_dims(tf.range(self.x_dim), 0), [batch_size_tensor, 1])\n",
    "        yy_range = tf.expand_dims(yy_range, -1)\n",
    "        yy_channel = tf.matmul(yy_range, yy_ones)\n",
    "        yy_channel = tf.expand_dims(yy_channel, -1)\n",
    "        xx_channel = tf.cast(xx_channel, 'float32') / (self.x_dim - 1)\n",
    "        yy_channel = tf.cast(yy_channel, 'float32') / (self.y_dim - 1)\n",
    "        xx_channel = xx_channel*2 - 1\n",
    "        yy_channel = yy_channel*2 - 1\n",
    "        ret = tf.concat([input_tensor, xx_channel, yy_channel], axis=-1)\n",
    "        if self.with_r:\n",
    "            rr = tf.sqrt( tf.square(xx_channel) + tf.square(yy_channel))\n",
    "            ret = tf.concat([ret, rr], axis=-1)\n",
    "        return ret\n",
    "\n",
    "class CoordConv(base.Layer):\n",
    "    \"\"\"CoordConv layer as in the paper.\"\"\"\n",
    "    def __init__(self, x_dim, y_dim, with_r, *args, **kwargs):\n",
    "        super(CoordConv, self).__init__()\n",
    "        self.addcoords = AddCoords(x_dim=x_dim,\n",
    "        y_dim=y_dim,\n",
    "        with_r=with_r)\n",
    "        self.conv = layers.Conv2D(*args, **kwargs)\n",
    "    def call(self, input_tensor):\n",
    "        ret = self.addcoords(input_tensor)\n",
    "        ret = self.conv(ret)\n",
    "        return ret    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecDj4Ra9ilnz"
   },
   "outputs": [],
   "source": [
    "addlayer = AddCoords()\n",
    "\n",
    "input1 = keras.Input(shape=(lb,lb,1), name=\"wap\")\n",
    "input2 = keras.Input(shape=(lb,lb,1), name=\"open\")\n",
    "input3 = keras.Input(shape=(lb,lb,1), name=\"shl\")\n",
    "input4 = keras.Input(shape=(lb,lb,1), name=\"sco\")\n",
    "input5 = keras.Input(shape=(lb,lb,1), name=\"trades\")\n",
    "inputs = layers.Concatenate(axis=-1)([input1, input2, input3, input4, input5])\n",
    "inputs = addlayer(inputs)\n",
    "x = layers.Conv2D(32, 3, activation='relu', input_shape=(lb,lb,7))(inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.AveragePooling2D((2,2))(x)\n",
    "x = layers.Conv2D(128, 5, activation='relu')(x)\n",
    "x = layers.Conv2D(128, 10, activation='relu')(x)\n",
    "x = layers.AveragePooling2D((2,2))(x)\n",
    "x = layers.Conv2D(256, 15, activation='relu')(x)\n",
    "x = layers.Conv2D(256, 15, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='tanh')(x)\n",
    "x = layers.Dense(64, activation='tanh')(x)\n",
    "x = layers.Dense(32, activation='tanh')(x)\n",
    "x = layers.Dense(16, activation='tanh')(x)\n",
    "output = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "seer = keras.Model([input1, input2, input3, input4, input5], output, name='seer')\n",
    "seer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INw4j84WiwFX"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 1e-4\n",
    "seer.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIkv0iZCYMdp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q14D4sPBJyoS"
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "#Uncomment the line below when using pre-trained weights \n",
    "#model.load_weights(checkpoint_path)\n",
    "\n",
    "for com in companies:\n",
    "    model_in = np.load(f\"input {com[:-4]}.npy\")\n",
    "    wap = model_in[:,:,:,0].reshape((-1, lb, lb, 1))\n",
    "    open = model_in[:,:,:,1].reshape((-1, lb, lb, 1))\n",
    "    shl = model_in[:,:,:,2].reshape((-1, lb, lb, 1))\n",
    "    sco = model_in[:,:,:,3].reshape((-1, lb, lb, 1))\n",
    "    trades = model_in[:,:,:,4].reshape((-1, lb, lb, 1))\n",
    "    model_out = np.load(f\"output {com[:-4]}.npy\")\n",
    "\n",
    "    print(f\"Training on {com[:-4]} data\")\n",
    "\n",
    "    history.append(seer.fit([wap, open, shl, sco, trades], model_out, epochs=15, validation_split=0.2, verbose=2, \n",
    "                        callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, min_delta=1e-5),\n",
    "                        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",min_delta=0.0005, factor=0.5, patience=3, min_lr=1e-6),\n",
    "                        keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_best_only=True)]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5D9TfDwpnJH"
   },
   "outputs": [],
   "source": [
    "for i in range(len(companies)):\n",
    "    acc = history[i].history['accuracy']\n",
    "    val_acc = history[i].history['val_accuracy']\n",
    "\n",
    "    loss = history[i].history['loss']\n",
    "    val_loss = history[i].history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hmH79jC2USd"
   },
   "outputs": [],
   "source": [
    "seer([wap[127].reshape((1,lb,lb,1)), open[127].reshape((1,lb,lb,1)), shl[127].reshape((1,lb,lb,1))\n",
    "            , sco[127].reshape((1,lb,lb,1)), trades[127].reshape((1,lb,lb,1))]), model_out[127]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
