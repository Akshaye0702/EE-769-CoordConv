{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUeHox9JWYqf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def scaled_trainer(arr, lb, lf, choice):\n",
        "    sc = MinMaxScaler(feature_range=(-1,1))\n",
        "    tx = np.array([])\n",
        "    ty = np.array([])\n",
        "    t = np.array([])\n",
        "    tx_new = np.array([])\n",
        "\n",
        "    for i in arr:\n",
        "        data = pd.read_csv(i).filter([choice]).values\n",
        "        data = sc.fit_transform(data)\n",
        "        t = np.append(t, sliding_window_view(data, window_shape = (lb+lf,1)))\n",
        "\n",
        "    t = t.reshape(int(t.shape[0]/(lb+lf)), lb+lf)\n",
        "    for j in t:\n",
        "        tx = np.append(tx, j[:lb])\n",
        "        ty = np.append(ty, j[lb:])\n",
        "    tx = tx.reshape((int(tx.shape[0]/lb), lb))\n",
        "       \n",
        "    tx_new = tx_new.reshape((int(tx_new.shape[0]/(lb-1)), (lb-1)))\n",
        "    ty = ty.reshape((int(ty.shape[0]/lf), lf))\n",
        "\n",
        "    return tx, ty"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb=160\n",
        "lf=5\n",
        "#List of companies to train on:\n",
        "companies = [\"tata chem.csv\", \"rel power.csv\"]\n",
        "\n",
        "for com in companies:\n",
        "    trade_x, _ = scaled_trainer([com], lb, lf, \"No. of Trades\")\n",
        "    shl_x, _ = scaled_trainer([com], lb, lf, \"Spread High-Low\")\n",
        "    wap_x, wap_y = scaled_trainer([com], lb, lf, \"WAP\")\n",
        "    open_x, _y = scaled_trainer([com], lb, lf, \"Open Price\")\n",
        "    sco_x, _ = scaled_trainer([com], lb, lf, \"Spread Close-Open\")\n",
        "    \n",
        "    wap_img = np.array([])\n",
        "    shl_img = np.array([])\n",
        "    trade_img = np.array([])\n",
        "    sco_img = np.array([])\n",
        "    open_img = np.array([])\n",
        "    for i in range(len(wap_x)):\n",
        "        eps = 0.1\n",
        "        d1 = np.abs(np.subtract.outer(wap_x[i],wap_x[i]))\n",
        "        R1 = np.exp(-d1**2/eps)\n",
        "        wap_img = np.append(wap_img, [R1])\n",
        "        d2 = np.abs(np.subtract.outer(shl_x[i],shl_x[i]))\n",
        "        R2 = np.exp(-d2**2/eps)\n",
        "        shl_img = np.append(shl_img, [R2])\n",
        "        d3 = np.abs(np.subtract.outer(trade_x[i],trade_x[i]))\n",
        "        R3 = np.exp(-d3**2/eps)\n",
        "        trade_img = np.append(trade_img, [R3]) \n",
        "        d4 = np.abs(np.subtract.outer(sco_x[i],sco_x[i]))\n",
        "        R4 = np.exp(-d3**2/eps)\n",
        "        sco_img = np.append(sco_img, [R4]) \n",
        "        d5 = np.abs(np.subtract.outer(open_x[i],open_x[i]))\n",
        "        R5 = np.exp(-d3**2/eps)\n",
        "        open_img = np.append(open_img, [R5]) \n",
        "        \n",
        "    wap_img = wap_img.reshape((int(wap_img.shape[0]/(160*160)), 160, 160, 1))\n",
        "    shl_img = shl_img.reshape((int(shl_img.shape[0]/(160*160)), 160, 160, 1))\n",
        "    trade_img = trade_img.reshape((int(trade_img.shape[0]/(160*160)), 160, 160, 1))\n",
        "    sco_img = sco_img.reshape((int(sco_img.shape[0]/(160*160)), 160, 160, 1))\n",
        "    open_img = open_img.reshape((int(open_img.shape[0]/(160*160)), 160, 160, 1))\n",
        "\n",
        "    x = [1,2,3,4,5]\n",
        "    slopes = np.array([])\n",
        "    for i in wap_y:\n",
        "        slope, _ = np.polyfit(x, i, 1)\n",
        "        slopes = np.append(slopes, slope)\n",
        "\n",
        "    model_out = np.array([])\n",
        "    for i in slopes:\n",
        "        if i<(-0.01):\n",
        "            model_out = np.append(model_out, 0)\n",
        "        elif i<=(0.01):\n",
        "            model_out = np.append(model_out, 1)\n",
        "        else:\n",
        "            model_out = np.append(model_out, 2)\n",
        "    \n",
        "    bins = np.arange(0, 4)\n",
        "    hist, _ = np.histogram(slopes, bins=3)\n",
        "    print(hist)\n",
        "    plt.bar(bins[:-1], hist, align='center')\n",
        "    plt.xticks(bins[:-1])\n",
        "    plt.show()\n",
        "\n",
        "    model_in = wap_img\n",
        "    model_in = np.append(model_in, open_img, axis=3)\n",
        "    model_in = np.append(model_in, shl_img, axis=3)\n",
        "    model_in = np.append(model_in, sco_img, axis=3)\n",
        "    model_in = np.append(model_in, trade_img, axis=3)\n",
        "\n",
        "    np.save(f\"input {com[:-4]}\", model_in)\n",
        "    np.save(f\"output {com[:-4]}\", model_out)\n",
        "    print(f\"Saved data on {com[:-4]}: Input/Output\", model_in.shape, model_out.shape)"
      ],
      "metadata": {
        "id": "BCo2-NDIZDx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "input1 = keras.Input(shape=(160,160,1), name=\"wap\")\n",
        "input2 = keras.Input(shape=(160,160,1), name=\"open\")\n",
        "input3 = keras.Input(shape=(160,160,1), name=\"shl\")\n",
        "input4 = keras.Input(shape=(160,160,1), name=\"sco\")\n",
        "input5 = keras.Input(shape=(160,160,1), name=\"trades\")\n",
        "inputs = layers.Concatenate(axis=-1)([input1, input2, input3, input4, input5])\n",
        "inputs = addlayer(inputs)\n",
        "x = layers.Conv2D(32, 3, activation='relu', input_shape=(160,160,7))(inputs)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "x = layers.AveragePooling2D((2,2))(x)\n",
        "x = layers.Conv2D(128, 5, activation='relu')(x)\n",
        "x = layers.Conv2D(128, 10, activation='relu')(x)\n",
        "x = layers.AveragePooling2D((2,2))(x)\n",
        "x = layers.Conv2D(256, 15, activation='relu')(x)\n",
        "x = layers.Conv2D(256, 15, activation='relu')(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='tanh')(x)\n",
        "x = layers.Dense(64, activation='tanh')(x)\n",
        "x = layers.Dense(32, activation='tanh')(x)\n",
        "x = layers.Dense(16, activation='tanh')(x)\n",
        "output = layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "seer = keras.Model([input1, input2, input3, input4, input5], output, name='seer')\n",
        "seer.summary()"
      ],
      "metadata": {
        "id": "6nU2-tz6ZTRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 1e-4\n",
        "seer.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oeimkZCLZVlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "checkpoint_path = \"training/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n"
      ],
      "metadata": {
        "id": "7RiK_pCVZY4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "#Uncomment the line below when using pre-trained weights \n",
        "#model.load_weights(checkpoint_path)\n",
        "\n",
        "for com in companies:\n",
        "    model_in = np.load(f\"input {com[:-4]}.npy\")\n",
        "    wap = model_in[:,:,:,0].reshape((-1, 160, 160, 1))\n",
        "    open = model_in[:,:,:,1].reshape((-1, 160, 160, 1))\n",
        "    shl = model_in[:,:,:,2].reshape((-1, 160, 160, 1))\n",
        "    sco = model_in[:,:,:,3].reshape((-1, 160, 160, 1))\n",
        "    trades = model_in[:,:,:,4].reshape((-1, 160, 160, 1))\n",
        "    model_out = np.load(f\"output {com[:-4]}.npy\")\n",
        "\n",
        "    print(f\"Training on {com[:-4]} data\")\n",
        "\n",
        "    history.append(seer.fit([wap, open, shl, sco, trades], model_out, epochs=15, validation_split=0.2, verbose=2, \n",
        "                        callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, min_delta=1e-5),\n",
        "                        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",min_delta=0.0005, factor=0.5, patience=3, min_lr=1e-6),\n",
        "                        keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_best_only=True)]))\n",
        "    \n"
      ],
      "metadata": {
        "id": "VLlGiFhlZa7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(companies)):\n",
        "    acc = history[i].history['accuracy']\n",
        "    val_acc = history[i].history['val_accuracy']\n",
        "\n",
        "    loss = history[i].history['loss']\n",
        "    val_loss = history[i].history['val_loss']\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "umH4hjiVZkjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seer([wap[127].reshape((1,160,160,1)), open[127].reshape((1,160,160,1)), shl[127].reshape((1,160,160,1))\n",
        "            , sco[127].reshape((1,160,160,1)), trades[127].reshape((1,160,160,1))]), model_out[127]"
      ],
      "metadata": {
        "id": "Fe8roRcqZqEs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}